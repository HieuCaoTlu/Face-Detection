import { useEffect, useRef, useState } from "react";
import * as faceapi from "face-api.js";
import { Button, Typography, Box, CircularProgress } from "@mui/material";
import { useSnackbar } from "../context/snackbar_context/useSnackbar";
import { useAuth } from "../context/auth_context/useAuth";

export default function CameraComponent() {
    const { showSnackbar } = useSnackbar();
    const videoRef = useRef(null);
    const canvasRef = useRef(null);
    const [stream, setStream] = useState(null);
    const [error, setError] = useState(null);
    const [capturedImages, setCapturedImages] = useState([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [timer, setTimer] = useState(5); // 10 gi√¢y quay video
    const [message, setMessage] = useState("Quay video, xoay m·∫∑t m·ªói 2s!");
    const { user } = useAuth()


    useEffect(() => {
        const loadModels = async () => {
            const MODEL_URL = "/models";
            try {
                await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
                console.log("‚úÖ Models loaded!");
            } catch (error) {
                console.error("‚ùå Error loading models:", error);
            }
        };
        loadModels();
    }, []);

    useEffect(() => {
        return () => {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
        };
    }, [stream]);

    const startCamera = async () => {
        setCapturedImages([]);
        setIsProcessing(false);
        setTimer(10);
        setMessage("Quay video, xoay m·∫∑t m·ªói 2s!");

        try {
            const userStream = await navigator.mediaDevices.getUserMedia({ video: true });
            setStream(userStream);
            if (videoRef.current) {
                videoRef.current.srcObject = userStream;
                videoRef.current.onloadedmetadata = () => {
                    videoRef.current.play();
                    detectFaces();
                    startTimer();
                };
            }
        } catch (err) {
            setError(err);
        }
    };

    const startTimer = () => {
        const interval = setInterval(() => {
            setTimer(prev => {
                if (prev <= 1) {
                    clearInterval(interval);
                    stopCamera();
                    return 0;
                }
                return prev - 1;
            });
        }, 1000);
    };

    const captureImage = async (probability) => {
        if (capturedImages.length >= 5) return;

        const video = videoRef.current;
        if (!video) {
            console.error("Kh√¥ng th·ªÉ truy c·∫≠p videoRef.current");
            return;
        }

        const canvas = document.createElement("canvas");
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const ctx = canvas.getContext("2d");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        const imageData = canvas.toDataURL("image/png");

        console.log(`üì∏ ·∫¢nh ${capturedImages.length + 1} ch·ª•p v·ªõi x√°c su·∫•t: ${probability}`);
        console.log(`üñºÔ∏è Image Data URL:`, imageData);

        // D√πng h√†m callback ƒë·ªÉ ƒë·∫£m b·∫£o c·∫≠p nh·∫≠t ch√≠nh x√°c
        setCapturedImages(prev => {
            const newImages = [...prev, { image: imageData, prob: probability }];
            return newImages;
        });
    };

    useEffect(() => {
        console.log("Captured images have been updated:", capturedImages);
    }, [capturedImages]);

    const detectFaces = async () => {
        if (!videoRef.current || !canvasRef.current) return;

        const video = videoRef.current;
        const canvas = canvasRef.current;
        const displaySize = { width: video.videoWidth, height: video.videoHeight };
        faceapi.matchDimensions(canvas, displaySize);

        const interval = setInterval(async () => {
            // Nh·∫≠n di·ªán t·∫•t c·∫£ khu√¥n m·∫∑t
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions({ scoreThreshold: 0.5 }));

            // ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc c·ªßa c√°c k·∫øt qu·∫£ nh·∫≠n di·ªán v·ªõi k√≠ch th∆∞·ªõc c·ªßa video
            const resizedDetections = faceapi.resizeResults(detections, displaySize);
            const ctx = canvas.getContext("2d");
            ctx.clearRect(0, 0, canvas.width, canvas.height); // X√≥a canvas m·ªói l·∫ßn tr∆∞·ªõc khi v·∫Ω l·∫°i

            // V·∫Ω bounding box cho t·∫•t c·∫£ khu√¥n m·∫∑t ƒë√£ ph√°t hi·ªán
            faceapi.draw.drawDetections(canvas, resizedDetections);

            resizedDetections.forEach(detection => {
                if (detection.score >= 0.8) {
                    captureImage(detection.score); // Ch·ª•p ·∫£nh n·∫øu ƒë·ªô ch√≠nh x√°c ƒë·ªß cao
                }
            });

            // D·ª´ng nh·∫≠n di·ªán sau khi ƒë√£ ch·ª•p ƒë·ªß ·∫£nh (ho·∫∑c khi h·∫øt th·ªùi gian quay video)
            if (capturedImages.length >= 5 || timer <= 0) {
                clearInterval(interval);
            }
        }, 200); // C·∫≠p nh·∫≠t bounding box v√† nh·∫≠n di·ªán m·ªói 200ms
    };

    const stopCamera = () => {
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
        }
        setStream(null);
        setIsProcessing(true);
        // Di chuy·ªÉn showSnackbar v√†o useEffect
        setTimeout(() => {
            showSnackbar("Qu√° tr√¨nh quay video k·∫øt th√∫c!", "success");
        }, 0);

        // G·ª≠i d·ªØ li·ªáu ·∫£nh
        const formData = new FormData();
        // images.forEach((file) => {
        //     formData.append("images[]", file);  // G·ª≠i t·∫•t c·∫£ ·∫£nh v√†o m·ªôt key "images[]"
        // });
        formData.append("label", user.name);
        console.log("Form data ƒë√£ chu·∫©n b·ªã:", formData);

        setIsProcessing(false);
    };

    return (
        <Box sx={{ display: "flex", flexDirection: "column", alignItems: "center", gap: 2 }}>
            {error && <Typography sx={{ color: "error.main" }}>{error.message}</Typography>}
            {isProcessing && <Typography variant="h6" sx={{ color: "success.main" }}>‚úÖ ƒê√£ x·ª≠ l√Ω ·∫£nh!</Typography>}
            <Box sx={{ position: "relative", width: "100%", maxWidth: 600 }}>
                {!isProcessing ? (
                    <>
                        <video ref={videoRef} autoPlay playsInline style={{ width: "100%", height: "auto", borderRadius: 2 }} />
                        <canvas ref={canvasRef} style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%", pointerEvents: "none" }} />
                    </>
                ) : (
                    <Typography variant="h6" sx={{ color: "success.main" }}>·∫¢nh ch·ª•p ƒë√£ xong!</Typography>
                )}
            </Box>
            <Typography>{message} {timer > 0 && `(${timer}s)`}</Typography>
            {!isProcessing && (
                <Button variant="contained" color="primary" onClick={startCamera} sx={{ padding: "10px 15px" }}>
                    M·ªü Camera
                </Button>
            )}
            {isProcessing && <CircularProgress />}
        </Box>
    );
}
